{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "iP8dZvCn6L2W"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "I6S_DkMA6L2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Hai Dinh, 19960331-4494, hasy@student.chalmers.se** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Christopher Meszaros, 19930707-2778,  meszaros@student.chalmers.se** <br />"
      ]
    },
    {
      "metadata": {
        "id": "iP8dZvCn6L2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Theoretical problems"
      ]
    },
    {
      "metadata": {
        "id": "kmJ93xEZ6goQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [Naive Bayes Classifier, 6 points]"
      ]
    },
    {
      "metadata": {
        "id": "RdIazgRp7KR6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
        "$$\n",
        "x = (\\mbox{rich, married, healthy})\n",
        "$$\n",
        "\n",
        "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
        "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
        "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
        "if they’re not. The following responses were obtained.\n",
        "\n",
        "$$\n",
        "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
        "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
        "$$\n",
        "\n",
        "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
        "\n",
        "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')"
      ]
    },
    {
      "metadata": {
        "id": "zSuIWu4t6sv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "<br>Let $(\\mathbf{x}, y)$ be a single observation, where $\\mathbf{x}$ is the feature vector, and $y$ is the class label for the observation. We define $P(x_i=v \\mid y=c)$ as the probability that the ith feature of the observation has the value $v$, given that the observation belongs to the class $c$. This probability is mathematically equal to the proportion of observations that have the ith feature equal to $v$ in the class $c$. We summarize these probabilities in the table below:\n",
        "\n",
        "<table border=\"1\" style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th></th>\n",
        "    <th>$y$ $=$ $0$</th>\n",
        "    <th>$y$ $=$ $1$</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_1$ $=$ $0$</td>\n",
        "    <td>$3$$/$$4$</td>\n",
        "    <td>$1$$/$$4$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_1$ $=$ $1$</td>\n",
        "    <td>$1$$/$$4$</td>\n",
        "    <td>$3$$/$$4$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_2$ $=$ $0$</td>\n",
        "    <td>$3$$/$$4$</td>\n",
        "    <td>$1$$/$$2$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_2$ $=$ $1$</td>\n",
        "    <td>$1$$/$$4$</td>\n",
        "    <td>$1$$/$$2$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_3$ $=$ $0$</td>\n",
        "    <td>$3$$/$$4$</td>\n",
        "    <td>$1$$/$$4$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_3$ $=$ $1$</td>\n",
        "    <td>$1$$/$$4$</td>\n",
        "    <td>$3$$/$$4$</td>\n",
        "  </tr>\n",
        "</table> \n",
        "\n",
        "<br>According to the Bayes' rules, we have the following equation for the probability of a person who is not rich, married and healthy to be *content*:\n",
        "$$ P(y=1\\mid \\mathbf{x}=(0,1,1)) \n",
        "   = \\dfrac{P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)P(y=1)}{P(\\mathbf{x}=(0,1,1))} \n",
        "   = \\dfrac{P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)P(y=1)}{P\\big(\\mathbf{x}=(0,1,1)\\mid y=0\\big)P(y=0) + P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)P(y=1)}\n",
        "$$\n",
        "\n",
        "<br>Since we have an equal number of observations in both class labels, we can expect the priors to be the same, with $P(c=0)=P(c=1)=1/2$. Plugging these priors into the equation above would cancel out the priors in both the numerator and the denominator, yielding the following:\n",
        "$$ P(y=1\\mid \\mathbf{x}=(0,1,1)) \n",
        "   = \\dfrac{P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)}{P\\big(\\mathbf{x}=(0,1,1)\\mid y=0\\big) + P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)} \n",
        "\\tag{1}$$\n",
        "\n",
        "<br>Naive Bayes makes the additional assumption that the features are all independent from one another. This means we can treat each feature individually, and then multiply their likelihoods. This gives us the following answer for the first question:\n",
        "$$ P\\big(\\mathbf{x}=(0,1,1)\\mid y=1\\big)\n",
        "   = P(x_1=0\\mid y=1)P(x_2=1\\mid y=1)P(x_3=1\\mid y=1)\n",
        "   = (1/4)(1/2)(3/4) = 3/32\n",
        "$$\n",
        "$$ P\\big(\\mathbf{x}=(0,1,1)\\mid y=0\\big)\n",
        "   = P(x_1=0\\mid y=0)P(x_2=1\\mid y=0)P(x_3=1\\mid y=0)\n",
        "   = (3/4)(1/4)(1/4) = 3/64\n",
        "$$\n",
        "$$ \\Leftrightarrow P(y=1\\mid \\mathbf{x}=(0,1,1)) \n",
        "   = \\dfrac{3}{32} \\div \\left( \\dfrac{3}{32} + \\dfrac{3}{64} \\right)\n",
        "   = \\dfrac{2}{3} \n",
        "\\tag{2}$$\n",
        "\n",
        "<br>For the second question, applying the Naive Bayes assumption on the likelihood gives:\n",
        "$$ P\\big(\\mathbf{x}=(0,1,\\_)\\mid y=c\\big) $$\n",
        "$$ = P\\big(\\mathbf{x}=(0,1,1)\\mid y=c\\big) + P\\big(\\mathbf{x}=(0,1,0)\\mid y=c\\big) $$\n",
        "$$ = P(x_1=0\\mid y=c)P(x_2=1\\mid y=c)\\big(P(x_3=1\\mid y=c) + P(x_3=0\\mid y=c)\\big) $$\n",
        "$$ = P(x_1=0\\mid y=c)P(x_2=1\\mid y=c) $$\n",
        "\n",
        "<br>This shows us that we can simply ignore the \"healthy\" feature, and the calculation would still be correct. So the answer for the second question is:\n",
        "$$ P\\big(\\mathbf{x}=(0,1,\\_)\\mid y=1\\big)\n",
        "   = P(x_1=0\\mid y=1)P(x_2=1\\mid y=1)\n",
        "   = (1/4)(1/2) = 1/8\n",
        "$$\n",
        "$$ P\\big(\\mathbf{x}=(0,1,\\_)\\mid y=0\\big)\n",
        "   = P(x_1=0\\mid y=0)P(x_2=1\\mid y=0)\n",
        "   = (3/4)(1/4) = 3/16\n",
        "$$\n",
        "$$ \\Leftrightarrow P(y=1\\mid \\mathbf{x}=(0,1,\\_)) \n",
        "   = \\dfrac{1}{8} \\div \\left( \\dfrac{1}{8} + \\dfrac{3}{16} \\right)\n",
        "   = \\dfrac{2}{5} \n",
        "\\tag{3}$$"
      ]
    },
    {
      "metadata": {
        "id": "0-h9RLKB6k2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [Extending Naive Bayes, 4 points]"
      ]
    },
    {
      "metadata": {
        "id": "Yg0GNLou7QDU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider now, the following vector of attributes:\n",
        "\n",
        "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
        "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
        "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
        "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
        "\n",
        "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset."
      ]
    },
    {
      "metadata": {
        "id": "m1xKfp1I6pmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "A potential difficulty is that the first 3 attributes ($x_1,x_2,x_3$) are dependent on each other, in the sense that exactly one of them must be true, while the other two must be false. This violates the assumption of Naive Bayes, which assumes that the features are independent from one another.\n",
        "\n",
        "The obvious way to fix this is to allow multi-value attributes instead of just binary attributes. This is done by combining the first 3 attributes into a single attribute:\n",
        "$$ x' = \\begin{cases} 0 &\\mbox{if } \\text{age} < 20 \\\\\n",
        "                      1 &\\mbox{if } 20 \\leq\\text{age}\\leq 30 \\\\\n",
        "                      1 &\\mbox{if } \\text{age} > 30 \\end{cases} $$\n",
        "\n",
        "The mathematics of the Naive Bayes method still holds for multi-value attributes, including the definition of $P(x_i=v \\mid y=c)$ presented above. So simply by merging the dependent attributes together, Naive Bayes works properly again, since the independence assumption is now valid. "
      ]
    },
    {
      "metadata": {
        "id": "esr3lbWq6L2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Practical problems"
      ]
    },
    {
      "metadata": {
        "id": "vKnKsnvT67-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [Bayes classifier, 5 points]"
      ]
    },
    {
      "metadata": {
        "id": "Py6H8qlk7ZPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
        "```python\n",
        "from numpy import genfromtxt\n",
        "data = genfromtxt('dataset2.txt', delimiter=',')\n",
        "labels = data[:,-1]\n",
        "```\n",
        "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
        "\n",
        "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
        "$$\n",
        "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
        "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
        "$$\n",
        "\n",
        "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
        "\n",
        "$$ \n",
        "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
        "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
        "$$\n",
        "\n",
        "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
        "\n",
        "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
        "\n",
        "```python\n",
        "def sph_bayes(Xtest, ...): # other parameters needed.\n",
        "\n",
        "    return [P1, P2, Ytest]\n",
        "```\n",
        "c. Write a function **new_classifier()**\n",
        "\n",
        "```python\n",
        "def new_classifier(Xtest, mu1, mu2)\n",
        "    \n",
        "    return [Ytest]\n",
        "```\n",
        "which implements the following classifier,\n",
        "$$\n",
        "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
        "$$\n",
        "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
        "\n",
        "d. Report 5-fold cross validation error for both classifiers."
      ]
    },
    {
      "metadata": {
        "id": "f3ywsJgXdKgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "Let $\\mathbb{I}\\{a\\}$ be the indicator that has the value 1 if $a$ is true and 0 otherwise. Since the class conditional density is spherical Gaussian, the MLE estimates for $\\mu$ and $\\sigma^2$ can be computed by using the formulae derived from the last homework. However, we need to make sure that we compute these estimates separately for each class label. Let $n_1$ be the number of observations in the first class $(+1)$, and $n_2$ be the number of observations in the second class $(-1)$. Then: \n",
        "$$\\hat{\\mu}_{1} = \\dfrac{1}{n_1}\\sum_{i=1}^n \\mathbf{x}_i \\cdot \\mathbb{I}\\{y_i=+1\\}\\tag{4}$$\n",
        "$$\\hat{\\mu}_{2} = \\dfrac{1}{n_2}\\sum_{i=1}^n \\mathbf{x}_i \\cdot \\mathbb{I}\\{y_i=-1\\}\\tag{5}$$\n",
        "$$\\hat{\\sigma}_{1}^2 = \\dfrac{1}{n_1p}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\mathsf{T}(\\mathbf{x}_i-\\mu) \\cdot \\mathbb{I}\\{y_i=+1\\}\\tag{6}$$\n",
        "$$\\hat{\\sigma}_{2}^2 = \\dfrac{1}{n_2p}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\mathsf{T}(\\mathbf{x}_i-\\mu) \\cdot \\mathbb{I}\\{y_i=-1\\}\\tag{7}$$\n",
        "\n",
        "Since the class conditional density is sphereical Gaussian, we can convert the multivariate Gaussian into the product of one-dimensional Guassian distributions of the features, exactly similar to slide 8 of the [lecture](http://www.cse.chalmers.se/research/lab/mlcourse/lecture4a_2018.pdf \"Lecture 4a\"). \n",
        "$$P(\\mathbf{x}_{new} \\mid y_{new}=+1,\\mathbf{X},y) \n",
        "  = \\left(\\dfrac{1}{\\hat{\\sigma}_1\\sqrt{2\\pi}}\\right)^{p}\n",
        "    \\exp\\left(-\\dfrac{1}{2\\hat{\\sigma}_1^2}\\sum_{i=1}^p\\left(\\mathbf{x}_i^{new}-\\hat{\\mu}_{1i}^2\\right)\\right) \n",
        "\\tag{8}$$\n",
        "$$P(\\mathbf{x}_{new} \\mid y_{new}=-1,\\mathbf{X},y) \n",
        "  = \\left(\\dfrac{1}{\\hat{\\sigma}_2\\sqrt{2\\pi}}\\right)^{p}\n",
        "    \\exp\\left(-\\dfrac{1}{2\\hat{\\sigma}_2^2}\\sum_{i=1}^p\\left(\\mathbf{x}_i^{new}-\\hat{\\mu}_{2i}^2\\right)\\right) \n",
        "\\tag{9}$$\n",
        "\n",
        "Finally, we simply use the Bayes' rules to calculate the posteriors. Note that since we assume the priors of the classes to be equal, the priors in the numerator and the denominator cancel each other out.<br>\n",
        "$$P(y_{new}=+1\\mid\\mathbf{x}_{new},X,y)\n",
        "  = \\dfrac{P(\\mathbf{x}_{new}\\mid y_{new}=+1,\\mathbf{X},y)}\n",
        "          {P(\\mathbf{x}_{new}\\mid y_{new}=+1,\\mathbf{X},y) + P(\\mathbf{x}_{new}\\mid y_{new}=-1,\\mathbf{X},y)} \n",
        "\\tag{10}$$<br>\n",
        "$$P(y_{new}=-1\\mid\\mathbf{x}_{new},X,y)\n",
        "  = \\dfrac{P(\\mathbf{x}_{new}\\mid y_{new}=-1,\\mathbf{X},y)}\n",
        "          {P(\\mathbf{x}_{new}\\mid y_{new}=+1,\\mathbf{X},y) + P(\\mathbf{x}_{new}\\mid y_{new}=-1,\\mathbf{X},y)} \n",
        "\\tag{11}$$\n",
        "\n",
        "Running the codes below, we see that both Bayes classifier and the new classifer perform equally well. In fact, they are 100% accurate. The reason is because the data in the dataset are highly separable. \n"
      ]
    },
    {
      "metadata": {
        "id": "Q98_G3P_72SC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "b93cce79-5a5c-4410-d3cf-da20117df31f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524507856886,
          "user_tz": -120,
          "elapsed": 3212,
          "user": {
            "displayName": "Hai Dinh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "105573796814574843650"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt, pi\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "### Uncomment to download the file containing the data\n",
        "!pip install -q wget\n",
        "import wget\n",
        "wget.download(\"http://www.cse.chalmers.se/research/lab/mlcourse/dataset2.txt\")\n",
        "\n",
        "def mle(Xtrain, ytrain, c):\n",
        "    \"\"\"\n",
        "    Returns MLE mean and sigma of a spherical Gaussian distribution, given the label.\n",
        "    @param c: The class label for which the mean and sigma is for.\n",
        "    \"\"\"\n",
        "    X = Xtrain[ytrain == c]\n",
        "    n, p = X.shape\n",
        "    mu = np.sum(X, axis=0) / n\n",
        "    M = np.square(X - mu)\n",
        "    sigma = sqrt(np.sum(M) / (n*p))\n",
        "    return mu, sigma\n",
        "\n",
        "def likelihood(Xtest, Xtrain, ytrain, c):\n",
        "    \"\"\"\n",
        "    Returns the likelihood for each data point in the test set, given the label.\n",
        "    @param c: The class label for which the likelihood is for.\n",
        "    \"\"\"\n",
        "    _,p = Xtrain.shape\n",
        "    mu, sigma = mle(Xtrain, ytrain, c)\n",
        "    L = np.square(Xtest - mu)\n",
        "    L = np.sum(L, axis=1) / (-2 * sigma**2)\n",
        "    L = np.exp(L) / (sigma * sqrt(2*pi))**p\n",
        "    return L\n",
        "  \n",
        "def sph_bayes(Xtest, Xtrain, ytrain):\n",
        "    \"\"\"\n",
        "    Returns the Baysian probabilities for each data point of being in class 1 and\n",
        "    in class 2, together with the predicted class labels for them.\n",
        "    \"\"\"\n",
        "    P1 = likelihood(Xtest, Xtrain, ytrain, 1)\n",
        "    P2 = likelihood(Xtest, Xtrain, ytrain, -1)\n",
        "    P1 = P1 / (P1 + P2)\n",
        "    P2 = 1 - P1\n",
        "    ytest = [-1 if p < 0.5 else 1 for p in P1]\n",
        "    return P1, P2, ytest\n",
        "\n",
        "def new_classifier(Xtest, Xtrain, ytrain):\n",
        "    \"\"\"\n",
        "    Returns the predictions using the new classifier defined in the text.\n",
        "    \"\"\"\n",
        "    mu1,_ = mle(Xtrain, ytrain, 1)\n",
        "    mu2,_ = mle(Xtrain, ytrain, -1)\n",
        "    mdif = mu1 - mu2\n",
        "    msum = mu1 + mu2\n",
        "    numer = np.inner(mdif, (Xtest - msum / 2))\n",
        "    denom = np.dot(mdif, mdif)\n",
        "    ytest = numer / denom\n",
        "    ytest = [-1 if y < 0 else 1 for y in ytest]\n",
        "    return ytest\n",
        "\n",
        "def accuracy(ytest, ypred):\n",
        "    \"\"\"\n",
        "    Returnns the accuracy of the predictions, compared to the original labels.\n",
        "    \"\"\"\n",
        "    count = np.count_nonzero(ytest == ypred)\n",
        "    return count / len(ytest)\n",
        "\n",
        "### Import the dataset\n",
        "data = np.genfromtxt('dataset2.txt', delimiter=',')\n",
        "X, y = data[:,:-1], data[:,-1]\n",
        "\n",
        "### Perform 5-fold cross validation on both classifiers\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
        "report = pd.DataFrame(columns=[\"sph_bayes()\", \"new_classifier()\"])\n",
        "for i, (train, test) in enumerate(skf.split(X, y), start=1):\n",
        "    _,_,ypred1 = sph_bayes(X[test], X[train], y[train])\n",
        "    ypred2 = new_classifier(X[test], X[train], y[train])\n",
        "    acc1 = accuracy(y[test], ypred1)\n",
        "    acc2 = accuracy(y[test], ypred2)\n",
        "    report.loc[\"Run \" + str(i)] = [acc1, acc2]\n",
        "\n",
        "### Report the result\n",
        "report.loc[\"Average\"] = report.mean(axis=0)\n",
        "print(\"Accuracies Report:\\n\", report, sep=\"\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies Report:\n",
            "         sph_bayes()  new_classifier()\n",
            "Run 1            1.0               1.0\n",
            "Run 2            1.0               1.0\n",
            "Run 3            1.0               1.0\n",
            "Run 4            1.0               1.0\n",
            "Run 5            1.0               1.0\n",
            "Average          1.0               1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vS3iombD7e9-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [DIGITS dataset classifer, 5 points]"
      ]
    },
    {
      "metadata": {
        "id": "n5EShX0n7klC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the DIGITS dataset:\n",
        "```python\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "```\n",
        "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
        "```python\n",
        "data = digits.data\n",
        "print(data.shape)\n",
        "target_names = digits.target_names\n",
        "print (target_names)\n",
        "import matplotlib.pyplot as plt\n",
        "y = digits.target\n",
        "plt.matshow(digits.images[0])\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
        "\n",
        "b. Investigate an alternative feature function as described below:\n",
        "\n",
        "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-16$). \n",
        "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
        "\n",
        "$$ \n",
        "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
        "$$\n",
        "\n",
        "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
        "$(b)$ in a single table. What can you say about the results?"
      ]
    },
    {
      "metadata": {
        "id": "Gk454eRcZS3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "As you can see from the results below, using the datasets with the full set of features (64 features) has a better accuracy compared the reduced set of features (16 features). This is no surprise, because reducing number of features loses the important information that help improve the classification method. Also digit 5 and 8 look kind of similar in terms of variances in rows and columns. This could also suggest that the classification between digit 5 and 8 may not benefit much from the alternative feature function, and it could even hurt the accuracy of the classification method, as we have already seen. "
      ]
    },
    {
      "metadata": {
        "id": "axIttta5TbJQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "53971839-0873-47d1-a2e4-1c9e18fb6ee4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524507860658,
          "user_tz": -120,
          "elapsed": 702,
          "user": {
            "displayName": "Hai Dinh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "105573796814574843650"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Load the dataset\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "data, labels, images = digits.data, digits.target, digits.images\n",
        "\n",
        "### Filter out all digits that are not 5 or 8\n",
        "index = (labels == 5) | (labels == 8)\n",
        "X,y,Z = data[index], labels[index], images[index]\n",
        "y = np.array([-1 if label == 5 else 1 for label in y])\n",
        "\n",
        "### Preparing the new features\n",
        "Z = Z / np.max(Z)\n",
        "rowvar = np.var(Z, axis=2)\n",
        "colvar = np.var(Z, axis=1)\n",
        "Z = np.concatenate((rowvar, colvar), axis=1)\n",
        "\n",
        "### Perform 5-fold cross validation on the 2 sets of features\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
        "report = pd.DataFrame(columns=[\"Part A\", \"Part B\"])\n",
        "for i, (train, test) in enumerate(skf.split(X, y), start=1):\n",
        "    ypredA = new_classifier(X[test], X[train], y[train])\n",
        "    ypredB = new_classifier(Z[test], Z[train], y[train])\n",
        "    accA = accuracy(y[test], ypredA)\n",
        "    accB = accuracy(y[test], ypredB)\n",
        "    report.loc[\"Run \" + str(i)] = [accA, accB]\n",
        "\n",
        "### Report the result\n",
        "report.loc[\"Average\"] = report.mean(axis=0)\n",
        "print(\"Accuracies Report:\\n\", report, sep=\"\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies Report:\n",
            "           Part A    Part B\n",
            "Run 1    0.958333  0.861111\n",
            "Run 2    0.972222  0.861111\n",
            "Run 3    1.000000  0.873239\n",
            "Run 4    1.000000  0.873239\n",
            "Run 5    0.985714  0.914286\n",
            "Average  0.983254  0.876597\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}